<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">  <meta name="description"
        content="Re-creating UI-JEPA with Audio Modality for Enhanced User Intent Understanding. A bimodal architecture that combines visual and auditory signals to improve intent inference.">
  <meta name="keywords" content="Audio UI JEPA, JEPA, ImageBind, User Intent, Multimodal, HCI">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Audio UI JEPA - Context Aware UI Intent Detection</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/IPhone_with_icons.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Audio UI JEPA - Context Aware UI Intent Detection</h1>          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://people.epfl.ch/sara.anejjar">Sara Anejjar</a> (329905)<sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://ugobalducci.fr">Ugo Balducci</a> (325035)<sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>EPFL</span>
            <span class="author-block" style="margin-left: 20px;"><em>CS-503 Final Project Report</em></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://go.epfl.ch/video-ui-jepa"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/saraanej/Audio-UI-JEPA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://ml-site.cdn-apple.com/datasets/ui-jepa/iit-1.0.0/data/raw.zip"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Audio UI JEPA</span> combines visual and auditory signals to understand user intent from mobile interface interactions.
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>        <div class="content has-text-justified">              <p>
                Understanding user intent in human-computer interaction (HCI) is critical for building adaptive and intelligent user interfaces. While vision-based approaches such as UI-JEPA <a href="#ref4">[4]</a> have demonstrated strong performance in modeling UI interaction sequences, they often overlook valuable audio context that can clarify ambiguous actions.
              </p>
              <p>
                In this work, we extend the UI-JEPA framework to incorporate audio data, forming a bimodal architecture—Audio-UI-JEPA—that combines visual and auditory signals to improve intent inference. Using pretrained joint embedding models (ImageBind <a href="#ref7">[7]</a>) and a lightweight language decoder (Phi-3 <a href="#ref6">[6]</a>), our system encodes and fuses UI video and synchronized audio to generate contextualized intent predictions.
              </p>
              <p>
                We evaluate our model on the IIT and IIW benchmarks, with augmented audio annotations, and show that Audio-UI-JEPA achieves higher accuracy in user intent prediction than its vision-only counterpart, particularly in tasks involving ambiguity or indirect cues. This work highlights the potential of multimodal learning for future AI-native UI systems.
              </p>
            </div>
          </div>
        </div>
        <!--/ Abstract. -->
      
        <!-- Paper video. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Video</h2>
            <div class="publication-video">
              <iframe src="https://go.epfl.ch/video-ui-jepa"
                      frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
          </div>
        </div>    <!--/ Paper video. -->
      </div>
    </div>
  </div>
</section>


<!-- Introduction -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Understanding user intent from interactions with digital interfaces is critical for advancing human-computer interaction (HCI). Existing systems rely heavily on heuristic-driven UI designs and reactive logic, limiting their ability to interpret complex or ambiguous user behaviors. With the proliferation of AI capabilities and increased demand for adaptive, personalized experiences, there is a pressing need for models that can understand user intent more accurately and contextually.
          </p>
          <p>
            Apple's UI-JEPA model <a href="#ref4">[4]</a> takes an important step in this direction by predicting intent from UI video sequences using a lightweight, self-supervised architecture based on a Vision Transformer (ViT) <a href="#ref5">[5]</a>. However, its reliance solely on visual data excludes valuable contextual cues often available in the surrounding audio—such as voice commands or background sounds—that can disambiguate user behavior. For instance, hearing a user say "Let's book a hotel" while they navigate through a travel app provides clear intent, even if the visual interaction is ambiguous.
          </p>
          <p>
            This project proposes a bimodal extension of UI-JEPA by incorporating an audio modality, forming a system we refer to as <em>Audio-UI-JEPA</em>. Our method fuses UI video sequences and corresponding audio input to predict user intent more robustly, using a JEPA-trained ViT encoder and a fine-tuned Phi-3 LLM <a href="#ref6">[6]</a>. The addition of audio is designed to address two research questions: (1) How can combining UI video and audio enhance intent prediction accuracy? and (2) What advantages does this bimodal system offer over a vision-only model in terms of performance and usability?
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Related Work -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Related Work</h2>
        <div class="content has-text-justified">
          <p>
            Understanding user intent from interaction data has been an ongoing challenge in HCI, tackled through a variety of machine learning and deep learning approaches. This section reviews relevant research in the areas of JEPA architectures, user intent prediction from UI interaction data, and multimodal representation learning.
          </p>
            <h4 class="title is-5">JEPA and V-JEPA</h4>
          <p>
            The Joint-Embedding Predictive Architecture (JEPA) <a href="#ref3">[3]</a>, proposed by LeCun et al., is a self-supervised learning framework aimed at learning predictive, abstract representations of future states in sequential data. Unlike traditional autoencoders that reconstruct pixel-level information, JEPA focuses on learning embeddings that are predictive of future states in a shared latent space. The Video-JEPA (V-JEPA) <a href="#ref2">[2]</a> variant extends this idea to video data, predicting masked visual patches from contextual embeddings without relying on pixel-wise loss, leading to more generalizable representations for downstream tasks.
          </p>
            <h4 class="title is-5">UI-JEPA</h4>
          <p>
            UI-JEPA <a href="#ref4">[4]</a> is a recent adaptation of JEPA to UI interaction sequences, where a Vision Transformer (ViT) <a href="#ref5">[5]</a> is trained to understand and predict user behavior from video captures of mobile app interactions. The model uses self-supervised learning with spatiotemporal masking, leveraging the predictiveness of latent states rather than reconstruction fidelity. While this model achieves strong results, its uni-modal nature (vision only) limits its interpretability in scenarios where audio context—such as speech or environmental cues—could help disambiguate user intent.
          </p>
            <h4 class="title is-5">ImageBind</h4>
          <p>
            ImageBind <a href="#ref7">[7]</a> is a multimodal embedding model developed by Meta AI that enables joint representation learning across six modalities, including images and audio. It achieves this by aligning different modality inputs in a shared embedding space, making it a powerful foundation for cross-modal retrieval and fusion tasks. We leverage ImageBind in Audio-UI-JEPA to bridge visual and audio inputs, producing aligned embeddings that can be decoded into natural language intent using a language model.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Method -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          
          <h4 class="title is-5">UI-JEPA Reproduction</h4>
          <p>
            To ensure reproducibility and a consistent experimental baseline, we first re-implemented the UI-JEPA model as described in the original paper. This step was critical for two reasons: (1) validating our ability to match the original model's performance, and (2) enabling us to later isolate the effect of introducing audio as an additional modality.
          </p>
          <p>
            After obtaining the IIW (Intent In the Wild) and IIT (Intent In the Tame) datasets from the Apple team, we reproduced the JEPA tuning process following the methodology outlined in the original work. These datasets consist of .mov video files of varying lengths. To standardize input, each video was sampled into 16 evenly spaced frames at a resolution of 384×384 pixels.
          </p>
          <p>
            Since the original paper did not specify the backbone model used, we selected <code>google/vit-base-patch16-384</code> <a href="#ref8">[8]</a>, a Vision Transformer (ViT) pretrained on ImageNet-21k and fine-tuned on ImageNet-1k at 384×384 resolution. This model, with 86 million parameters, was chosen to align with the hardware constraints stated in the paper—specifically, the requirement for mobile device compatibility.
          </p>
          
          <h4 class="title is-5">Audio-UI-JEPA Extension</h4>
          <p>
            To extend the model with audio data, we began by annotating the IIT dataset. This dataset includes 682 videos in the training split, 187 videos for few-shot evaluation, and 45 videos for zero-shot evaluation. We developed a custom UI annotation tool that enabled us to preview each video, record corresponding audio, and review it immediately to ensure alignment with the visual content.
          </p>
          <p>
            We opted for a joint embedding approach, leveraging <code>nielsr/imagebind-huge</code> <a href="#ref7">[7]</a>, a model trained to learn joint embeddings across six modalities: image, text, audio, depth, thermal, and IMU data. ImageBind processes audio by converting it into mel spectrograms, which allows us to apply the same masking and temporal preprocessing used in UI-JEPA—dividing inputs into 16 evenly spaced segments and applying the same masking strategy.
          </p>
        </div>
      </div>
    </div>
    
    <!-- Annotation Tool Figure -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <figure class="image">
          <img src="./static/images/annotation.svg" alt="Screenshot of the annotation GUI" style="max-width: 80%; height: auto;">
          <figcaption class="has-text-centered is-size-6" style="margin-top: 10px;">
            Screenshot of our custom annotation tool used to create audio annotations for the IIT dataset.
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<!-- Experiments -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiments</h2>
        <div class="content has-text-justified">
          <p>
            The UI-JEPA paper <a href="#ref4">[4]</a> established the first benchmarks for intent prediction using user interface (UI) videos. To evaluate performance, the authors used ROUGE-1, ROUGE-2, and ROUGE-L scores—metrics traditionally used in text generation tasks to measure the overlap between predicted and reference text at different granularities. In addition, they reported cosine similarity using Sentence-BERT (SBERT) <a href="#ref12">[12]</a>, a semantic similarity metric that evaluates how close the predicted intent is to the ground truth in an embedding space.
          </p>
          <p>
            To provide an aggregated view of model performance, the paper introduced the <strong>Intent Similarity</strong> score, computed by averaging the normalized values of the four metrics (SBERT, ROUGE-1, ROUGE-2, ROUGE-L), each rescaled to the range [0, 1]. This composite score captures both semantic alignment and textual overlap between predicted and true intents.
          </p>
          
          <h4 class="title is-5">Zero-shot Evaluation</h4>
          <div class="table-container">
            <table class="table is-bordered is-striped is-hoverable">
              <thead>
                <tr>
                  <th>Metric</th>
                  <th>UI-JEPA</th>
                  <th>Audio-UI-JEPA (Ours)</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong>SBERT</strong></td>
                  <td>41.91</td>
                  <td><strong>50.42</strong></td>
                </tr>
                <tr>
                  <td><strong>ROUGE-1</strong></td>
                  <td>29.81</td>
                  <td><strong>39.75</strong></td>
                </tr>
                <tr>
                  <td><strong>ROUGE-2</strong></td>
                  <td>11.66</td>
                  <td><strong>16.38</strong></td>
                </tr>
                <tr>
                  <td><strong>ROUGE-L</strong></td>
                  <td>27.55</td>
                  <td><strong>33.23</strong></td>
                </tr>
                <tr style="background-color: #f5f5f5;">
                  <td><strong>Intent Similarity</strong></td>
                  <td>34.99</td>
                  <td><strong>41.14</strong></td>
                </tr>
              </tbody>
            </table>
          </div>
          <p>
            In the zero-shot setting—where the model is evaluated on apps it has never seen during training—our model significantly outperforms UI-JEPA across all metrics. This improvement demonstrates that incorporating audio leads to more robust generalization. The large gain in SBERT (+8.5%) suggests our model better captures the semantic meaning of user intents.
          </p>
          
          <h4 class="title is-5">Few-shot Evaluation</h4>
          <div class="table-container">
            <table class="table is-bordered is-striped is-hoverable">
              <thead>
                <tr>
                  <th>Metric</th>
                  <th>UI-JEPA</th>
                  <th>Audio-UI-JEPA (Ours)</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong>SBERT</strong></td>
                  <td><strong>87.43</strong></td>
                  <td>80.24</td>
                </tr>
                <tr>
                  <td><strong>ROUGE-1</strong></td>
                  <td><strong>83.73</strong></td>
                  <td>74.67</td>
                </tr>
                <tr>
                  <td><strong>ROUGE-2</strong></td>
                  <td><strong>69.17</strong></td>
                  <td>57.13</td>
                </tr>
                <tr>
                  <td><strong>ROUGE-L</strong></td>
                  <td><strong>81.51</strong></td>
                  <td>71.69</td>
                </tr>
                <tr style="background-color: #f5f5f5;">
                  <td><strong>Intent Similarity</strong></td>
                  <td><strong>82.03</strong></td>
                  <td>73.40</td>
                </tr>
              </tbody>
            </table>
          </div>
          <p>
            In the few-shot evaluation—where the apps in test data overlap with those in training—our model underperforms compared to UI-JEPA. This drop could be due to UI-JEPA overfitting to the training distribution. However, our model achieves strong gains in zero-shot settings, proving its strength in generalization, which is crucial for deployment in real-world, unseen scenarios.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Conclusion -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusion and Limitations</h2>
        <div class="content has-text-justified">
          <p>
            In this work, we successfully reproduced Apple's UI-JEPA framework as described in their paper and extended it by introducing the audio modality. Our experiments demonstrate that this multimodal enhancement leads to significant improvements, particularly in the zero-shot setting. To the best of our knowledge, this represents the first attempt at building a multimodal JEPA-style model combining video and audio for intent prediction.
          </p>
          <p>
            However, our approach comes with notable limitations. First, our long-term goal is to deploy the model on smartphones, yet our current reliance on ImageBind may not align with the compute and memory constraints of mobile devices. Future work should explore more lightweight alternatives or model distillation techniques.
          </p>
          <p>
            Second, the quality of the training data is suboptimal. The IIT dataset includes user behaviors that do not always reflect realistic or purposeful interaction sequences. Furthermore, all audio annotations were created by the authors using a custom UI tool. Despite efforts to ensure objectivity and naturalness, annotator bias cannot be ruled out.
          </p>
          <p>
            To address these issues, future work should focus on collecting a large-scale, diverse dataset of real UI interactions with corresponding audio. Such a dataset would enable training a more robust and generalizable model. Further optimization is also needed for on-device inference, especially if deployment on smartphones is to be achieved.
          </p>
        </div>
      </div>
    </div>  </div>
</section>

<!-- References -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">References</h2>
        <div class="content has-text-justified">          <p class="reference" id="ref1">
            <strong>[1] VATT:</strong> Hassan Akbari et al. "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text." <em>arXiv preprint arXiv:2104.11178</em> (2021).
          </p>
          <p class="reference" id="ref2">
            <strong>[2] V-JEPA:</strong> Adrien Bardes et al. "Revisiting Feature Prediction for Learning Visual Representations from Video." <em>arXiv preprint arXiv:2404.08471</em> (2024).
          </p>
          <p class="reference" id="ref3">
            <strong>[3] JEPA:</strong> Yann LeCun. "A Path Towards Autonomous Machine Intelligence." Position paper.
          </p>
          <p class="reference" id="ref4">
            <strong>[4] UI-JEPA:</strong> Yicheng Fu et al. "UI-JEPA: Towards Active Perception of User Intent through Onscreen User Activity." <em>arXiv preprint arXiv:2409.04081</em> (2024).
          </p>
          <p class="reference" id="ref5">
            <strong>[5] Vision Transformer:</strong> Alexey Dosovitskiy et al. "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale." <em>CoRR abs/2010.11929</em> (2020).
          </p>
          <p class="reference" id="ref6">
            <strong>[6] Phi-3:</strong> Marah Abdin et al. "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone." <em>arXiv preprint arXiv:2404.14219</em> (2024).
          </p>
          <p class="reference" id="ref7">
            <strong>[7] ImageBind:</strong> Rohit Girdhar et al. "ImageBind: One Embedding Space To Bind Them All." <em>arXiv preprint arXiv:2305.05665</em> (2023).
          </p>
          <p class="reference" id="ref8">
            <strong>[8] Google ViT:</strong> Google. "google/vit-base-patch16-384." Hugging Face Model Hub.
          </p>
          <p class="reference" id="ref9">
            <strong>[9] Microsoft Phi-3:</strong> Microsoft. "Phi-3 Mini-4K-Instruct." Hugging Face Model Hub.
          </p>
          <p class="reference" id="ref10">
            <strong>[10] A-JEPA:</strong> Zhengcong Fei et al. "A-JEPA: Joint-Embedding Predictive Architecture Can Listen." <em>arXiv preprint arXiv:2311.15830</em> (2024).
          </p>
          <p class="reference" id="ref11">
            <strong>[11] JEPA Architecture:</strong> Mahmoud Assran et al. "Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture." <em>arXiv preprint arXiv:2301.08243</em> (2023).
          </p>
          <p class="reference" id="ref12">
            <strong>[12] Sentence-BERT:</strong> Nils Reimers and Iryna Gurevych. "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks." <em>arXiv preprint arXiv:1908.10084</em> (2019).
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">  <div class="container is-max-desktop content">    <h2 class="title">BibTeX</h2>
    <pre><code>@article{anejjar2025audio,
  author    = {Anejjar, Sara and Balducci, Ugo},
  title     = {Re-creating UI-JEPA with Audio Modality for Enhanced User Intent Understanding},
  journal   = {CS-503 Final Project Report},
  year      = {2025},
  institution = {EPFL}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/saraanej/Audio-UI-JEPA" class="external-link">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
